{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e35585a8-b05a-420e-ab07-9f4bfb57e2d3",
   "metadata": {},
   "source": [
    "# Creating a Parquet dataset from a combined counts table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9112f370-bfff-41ac-887e-13ffc639f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "from collections import namedtuple\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb686031-f1c3-4573-92cc-80556524f951",
   "metadata": {},
   "source": [
    "## Definitions of file paths, names, and splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106646cc-7b2a-4fcf-bc3c-1c93586ff9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path('/hpc/group/igvf')\n",
    "DB_ROOT = PROJECT_ROOT / 'db'\n",
    "STARR_DS = 'K562'\n",
    "DATA_ROOT = Path(f'../igvf-pm/{STARR_DS}/leave-one-out')\n",
    "STARR_DB = DB_ROOT / f\"{STARR_DS}db\"\n",
    "\n",
    "# datafile = \"K562.combined.sample.tsv.gz\"\n",
    "datafile = \"combined.input_and_output.gt_100.log2FC.sequence.txt.gz\"\n",
    "splits = [\"train\", \"test\"]\n",
    "fastafiles = pd.Series(\n",
    "    [f\"data-normalized/downsampled-{split}.fasta\" for split in splits],\n",
    "    index=splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34af7309-9a48-4007-b5d8-8000a82dad4d",
   "metadata": {},
   "source": [
    "## Read combined counts table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0887629f-ea16-4200-9f6c-10e8ef1b4d71",
   "metadata": {},
   "source": [
    "We read the combined counts table into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6e5bef1-4b80-4d8a-9d04-4478d1f3d956",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_ROOT / datafile, compression=\"gzip\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4665af1-a210-44bf-bfe1-caeadaa99fcd",
   "metadata": {},
   "source": [
    "The 1-based row number seems to be used as an identifier, so we preserve it as a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f03b171-f36b-4916-a3f5-04ab0492ad57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"index\"] = range(1, df.shape[0]+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb28a21d-7e02-4994-8d93-88d0adb5fe6e",
   "metadata": {},
   "source": [
    "## Read sampling of rows for different splits\n",
    "\n",
    "A sample consists of a counts table and a FASTA sequence bins file, both in the same order. We can recover the row identifier from the FASTA sequence bins file as the sequence identifier; the format of the sampled counts table no longer includes the row identifier.\n",
    "\n",
    "To enable verifying that a sequence bin is the same as in the combined counts table, in addition to the row identifier we extract and return chromosome, start and end. We also convert the four values to their correct types (`int` for index, start and end), and code this as a generator that returns a named tuple for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32e6f385-99a8-4514-9c60-c8034a036d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasta_bins(fastafile, compression=None):\n",
    "    rowtuple = namedtuple('rowtuple',[\"index\",\"chrom\",\"start\",\"end\"])\n",
    "    typeconvs = [int, str, int, int]\n",
    "    fopen = open\n",
    "    if compression:\n",
    "        fopen = gzip.open\n",
    "    with fopen(fastafile, \"rt\") as f:\n",
    "        pat = re.compile(r'>(\\d+)\\s+.*=(chr[0-9MXY]+):(\\d+)-(\\d+)')\n",
    "        for line in f:\n",
    "            if line.startswith(\">\"):\n",
    "                match = pat.match(line)\n",
    "                yield rowtuple(*[conv(val) for conv, val in zip(typeconvs, match.groups())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293a07d9-79a0-4e9b-8607-168688ea5386",
   "metadata": {},
   "source": [
    "We can now directly create a dataframe from the generator, with columns correctly named and of the right type. For a given split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6046298-b0a8-4442-9d9b-61b101750ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_split_from_fasta(df, fastafile, split, compression=None, check_cols=[\"chrom\",\"start\",\"end\"]):\n",
    "    df_split = pd.DataFrame(fasta_bins(fastafile, compression=compression))\n",
    "    df_split.index = df_split[\"index\"] - 1\n",
    "    if check_cols and len(check_cols) > 0:\n",
    "        check = df.loc[df_split.index, check_cols].eq(df_split.loc[:, check_cols]).all(axis=0).all()\n",
    "        if not check:\n",
    "            raise Exception(f\"Index mismatch with split '{split}' in {fastafile}\")\n",
    "    df[split] = False\n",
    "    df.loc[df_split.index, split] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dd2b69-70a2-4294-b6bf-0c925a7a997d",
   "metadata": {},
   "source": [
    "Now we can use this for each split we're dealing with, resulting in one additional column of type `bool` for each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bebc3e3-16d6-4f70-8f5e-fdc5d32d6e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining split 'train'\n",
      "Obtaining split 'test'\n"
     ]
    }
   ],
   "source": [
    "for split in splits:\n",
    "    print(f\"Obtaining split '{split}'\")\n",
    "    append_split_from_fasta(df, DATA_ROOT / fastafiles[split], split=split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ac4426-1e36-4f0e-afac-d7d802051791",
   "metadata": {},
   "source": [
    "Report absolute and relative sizes of the splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7107a139-d7a5-4055-b485-d33b64933a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 'train' is size 2600332 (0.13% of total)\n",
      "Split 'test' is size 2598989 (0.13% of total)\n"
     ]
    }
   ],
   "source": [
    "splitsizes = df.loc[:,splits].sum(axis=0)\n",
    "for split in splits:\n",
    "    print(f\"Split '{split}' is size {splitsizes[split]} ({splitsizes[split] / len(df):.2}% of total)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef4a8d9-2978-4238-9973-0915ea0d27f7",
   "metadata": {},
   "source": [
    "## Write Parquet dataset\n",
    "\n",
    "To write the dataframe to a Parquet dataset, we first need to a convert a `pyarrow.Table` object, which we then write using HIVE partitioning by chromosome and the two splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fad0f23-cf61-4525-b418-d5e1b720b199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_db(df, db_path, partition_cols=[\"chrom\",\"train\",\"test\"]):\n",
    "    dt = pa.Table.from_pandas(df)\n",
    "    pq.write_to_dataset(dt, root_path=db_path, partition_cols=partition_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73e7b0d-a957-405a-b397-5762029fd23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_db(df, db_path=STARR_DB)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
